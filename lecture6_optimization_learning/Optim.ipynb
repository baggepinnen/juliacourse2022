{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General optimization\n",
    "[Optim.jl](https://github.com/JuliaNLSolvers/Optim.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     7.645684e-21\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     BFGS\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 3.48e-07 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 3.48e-07 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 6.91e-14 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 9.03e+06 ≰ 0.0e+00\n",
       "    |g(x)|                 = 2.32e-09 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    16\n",
       "    f(x) calls:    53\n",
       "    ∇f(x) calls:   53\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pkg; Pkg.activate(\".\")\n",
    "using Optim\n",
    "rosenbrock(x) =  (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "result = optimize(rosenbrock, zeros(2), BFGS(), autodiff=:forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Optim automatically uses forward-mode AD to find the gradients of the cost-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`fminsearch` in matlab is similar to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     3.525527e-09\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     Nelder-Mead\n",
       "\n",
       " * Convergence measures\n",
       "    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    60\n",
       "    f(x) calls:    117\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = optimize(rosenbrock, zeros(2), NelderMead())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     3.081488e-31\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     Newton's Method\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 3.06e-09 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 3.06e-09 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 9.34e-18 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 3.03e+13 ≰ 0.0e+00\n",
       "    |g(x)|                 = 1.11e-15 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    14\n",
       "    f(x) calls:    44\n",
       "    ∇f(x) calls:   44\n",
       "    ∇²f(x) calls:  14\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = optimize(rosenbrock, zeros(2), Newton(), autodiff=:forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Optim supports a number of common optimization algorithms\n",
    "\n",
    "- Gradient Free\n",
    "  - Nelder Mead\n",
    "  - Simulated Annealing\n",
    "  - Simulated Annealing w/ bounds\n",
    "  - Particle Swarm\n",
    "- Gradient Required\n",
    "  - Conjugate Gradient\n",
    "  - Gradient Descent\n",
    "  - (L-)BFGS\n",
    "  - Acceleration\n",
    "- Hessian Required\n",
    "  - Newton\n",
    "  - Newton with Trust Region\n",
    "  - Interior point Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Optim is the goto tool if you want to optimize a Julia function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Drawbacks:\n",
    "- Only handles box constraints (not very well) and some simple manifolds\n",
    "- Does not exploit structure\n",
    "- Does not explot convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Pros:\n",
    "- Very easy to use\n",
    "- Automatic AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Convex program -> Convex.jl\n",
    "- Structured/difficult/mixed-integer problem -> JuMP.jl\n",
    "- Nonlinear constraints -> NLopt.jl\n",
    "- Deep learning -> Flux.jl"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
