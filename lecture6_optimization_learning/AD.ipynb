{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automatic Differentiation\n",
    "AD is an extremely powerful tool. In julia, you can differentiate almost any valid julia program to obtain derivatives, gradients, jacobians and hessians etc. automatically, with high performance. This is what makes up the bulk of most deep-learning libraries, but contrary to most libraries, you do not need to write your code using a subset of julia or a DL-specific language, you can just write regular julia code.\n",
    "\n",
    "There are a number of different kinds of AD. In the following, we will refer to a function \n",
    "$$ f : \\mathbb{R}^n -> \\mathbb{R}^m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward-mode AD\n",
    "Using dual numbers, forward-mode AD performs a single forward pass of your program, calculating both the function value and gradients in one go. FAD is algorithmically favorable when $f$ is \"few to many\", or $n < m$. It also typically has the least overhead, so is competitive when both $n$ and $m$ are small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reverse-mode AD\n",
    "This is what is used in DL libraries. RAD works by constructing a computation graph, either before execution (as old tensorflow) or during the execution (most common today).\n",
    "RAD is algorithmically favorable when $f$ is \"many to few\", or $n > m$. This is the case in most DL, where the cost function is a scalar-valued function of very many parameters, the NN weights. For functions with many outputs, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/repos/juliacourse2022/lecture6_optimization_learning`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " 1.1232106411559188\n",
       " 0.9836039492002165\n",
       " 0.8391388924682177\n",
       " 1.0341481020949057\n",
       " 0.9385659002702967"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pkg; Pkg.activate(\".\")\n",
    "using ForwardDiff, BenchmarkTools\n",
    "\n",
    "f(x) = sum(sin, x) + prod(tan, x) * sum(sqrt, x);\n",
    "\n",
    "x = rand(5) # small size \n",
    "g = x -> ForwardDiff.gradient(f, x); # g = ∇f\n",
    "g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  389.748 ns (4 allocations: 848 bytes)\n"
     ]
    }
   ],
   "source": [
    "@btime g($x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " -0.0151824   0.421806   0.302021   0.555813   0.359894\n",
       "  0.421806   -0.307795   0.121435   0.223572   0.14471\n",
       "  0.302021    0.121435  -0.535026   0.160064   0.103596\n",
       "  0.555813    0.223572   0.160064  -0.19853    0.19074\n",
       "  0.359894    0.14471    0.103596   0.19074   -0.392262"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ForwardDiff.hessian(f, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Zygote\n",
    "\n",
    "f'(x) ≈ g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.672 μs (30 allocations: 1.45 KiB)\n"
     ]
    }
   ],
   "source": [
    "@btime f'($x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we change the size of the input vector, the relative timings change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50.424 ms (6 allocations: 549.25 KiB)\n"
     ]
    }
   ],
   "source": [
    "x = rand(5000)\n",
    "@btime g($x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  106.030 μs (39 allocations: 469.64 KiB)\n"
     ]
    }
   ],
   "source": [
    "@btime f'($x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Most AD (except for Zygote and Yota) in julia works by overloading `Base` functions on custom types. This means that you can not use AD if you restrict the input types to your functions too much! In the following example, the input is restricted to `Vector{Float64}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104.18089589896489"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = abs.(randn(3))\n",
    "f2(x::Vector{Float64}) = sum(sin, x) + prod(tan, x) * sum(sqrt, x);\n",
    "f2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching f2(::Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(f2), Float64}, Float64, 3}})\n\u001b[0mClosest candidates are:\n\u001b[0m  f2(\u001b[91m::Vector{Float64}\u001b[39m) at In[30]:2",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching f2(::Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(f2), Float64}, Float64, 3}})\n\u001b[0mClosest candidates are:\n\u001b[0m  f2(\u001b[91m::Vector{Float64}\u001b[39m) at In[30]:2",
      "",
      "Stacktrace:",
      " [1] vector_mode_dual_eval!(f::typeof(f2), cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{typeof(f2), Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(f2), Float64}, Float64, 3}}}, x::Vector{Float64})",
      "   @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/apiutils.jl:37",
      " [2] vector_mode_gradient(f::typeof(f2), x::Vector{Float64}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{typeof(f2), Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(f2), Float64}, Float64, 3}}})",
      "   @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/gradient.jl:106",
      " [3] gradient(f::Function, x::Vector{Float64}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{typeof(f2), Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(f2), Float64}, Float64, 3}}}, ::Val{true})",
      "   @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/gradient.jl:19",
      " [4] gradient(f::Function, x::Vector{Float64}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{typeof(f2), Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(f2), Float64}, Float64, 3}}}) (repeats 2 times)",
      "   @ ForwardDiff ~/.julia/packages/ForwardDiff/pDtsf/src/gradient.jl:17",
      " [5] top-level scope",
      "   @ In[31]:1",
      " [6] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [7] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "ForwardDiff.gradient(f2, x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This did not work, since ForwardDiff  calls the function with an argument of type `Vector{<: Dual}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f2'(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This works since Zygote does not use dispatch on custom types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f3(x::Vector) = sum(sin, x) + prod(tan, x) * sum(sqrt, x);\n",
    "ForwardDiff.gradient(f3, x)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
